#!/usr/bin/env python3
"""
call_openai_parse.py

Reads an ai_input.json (generated by reduce4ai.py) and calls OpenAI to produce a
strict xAIO/WP-ready JSON output (xaio_parsed.json) using Structured Outputs.

- Uses Responses API + Structured Outputs via OpenAI SDK "responses.parse"
- Builds enums dynamically from your SCF export JSON (select field choices)
- Writes parsed output JSON + optional raw response JSON for debugging

Usage:
  python call_openai_parse.py out_ai/XYZ.ai_input.json
  python call_openai_parse.py out_ai/XYZ.ai_input.json --model gpt-5-nano
  python call_openai_parse.py out_ai/XYZ.ai_input.json --scf-export /path/to/scf-export.json

Notes:
- This stage does NOT do fact-checking. It produces:
  (a) narrative / summary-ish fields
  (b) atomic claims list with verdict="unverified"
  (c) quality/style scores (about writing quality + traceability, not truth)
"""

from __future__ import annotations

import argparse
import json
import logging
import re
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Type

from openai import APIConnectionError, APIError, APITimeoutError, OpenAI, RateLimitError
from pydantic import BaseModel, Field
from tenacity import (
    before_sleep_log,
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

from env_bootstrap import load_repo_env
from logging_utils import elapsed_ms, log_event, setup_logging
from openai_compat import structured_parse

load_repo_env()

logger = setup_logging("call_openai_parse")


# ---------------------------
# SCF export parsing (choices/enums)
# ---------------------------

def load_json(path: Path) -> Any:
    return json.loads(path.read_text(encoding="utf-8"))

def find_select_choices(scf_export: Any, field_name: str) -> Dict[str, str]:
    """
    SCF export is a list of field groups; some fields are nested in repeater sub_fields.
    We search both top-level fields and sub_fields.
    """
    if not isinstance(scf_export, list):
        return {}

    for group in scf_export:
        fields = group.get("fields", []) if isinstance(group, dict) else []
        for f in fields:
            if not isinstance(f, dict):
                continue
            if f.get("name") == field_name and f.get("type") == "select":
                return f.get("choices", {}) or {}
            if f.get("type") in ("repeater", "group"):
                for sf in f.get("sub_fields", []) or []:
                    if isinstance(sf, dict) and sf.get("name") == field_name and sf.get("type") == "select":
                        return sf.get("choices", {}) or {}
    return {}

def scf_enums_from_export(scf_export_path: Path) -> Dict[str, List[str]]:
    scf = load_json(scf_export_path)

    content_mode = sorted(list(find_select_choices(scf, "content_mode").keys()))
    language = sorted(list(find_select_choices(scf, "language").keys()))
    workflow_status = sorted(list(find_select_choices(scf, "workflow_status").keys()))
    intake_kind = sorted(list(find_select_choices(scf, "intake_kind").keys()))

    claim_type = sorted(list(find_select_choices(scf, "claim_type").keys()))
    verdict = sorted(list(find_select_choices(scf, "verdict").keys()))

    # Fall back to permissive defaults if export missing (shouldn’t happen)
    if not content_mode:
        content_mode = ["text_news_article", "text_other", "video_other", "audio_other", "unknown"]
    if not language:
        language = ["en", "unknown"]
    if not claim_type:
        claim_type = ["event", "date_time", "quantity", "quote", "attribution", "location", "policy_law", "identity", "causal", "other"]
    if not verdict:
        verdict = ["unverified", "true", "false", "mixed", "unverifiable", "disputed"]

    return {
        "content_mode": content_mode,
        "language": language,
        "workflow_status": workflow_status or ["pending_review", "needs_metadata", "needs_transcript", "needs_source_links", "approved", "rejected"],
        "intake_kind": intake_kind or ["text_first", "media_first", "unknown"],
        "claim_type": claim_type,
        "verdict": verdict,
    }


# ---------------------------
# Minimal coercion / hygiene
# ---------------------------

def clamp_score(x: Optional[float]) -> Optional[float]:
    if x is None:
        return None
    try:
        v = float(x)
    except Exception:
        return None
    if v < 0:
        return 0.0
    if v > 100:
        return 100.0
    return v

def normalize_claim_text(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"\s+", " ", s)
    # avoid trailing punctuation spam
    return s.strip()


# ---------------------------
# Pydantic schema for Structured Outputs
# ---------------------------

def build_models(enums: Dict[str, List[str]]) -> Tuple[Type[BaseModel], Type[BaseModel]]:
    """
    Builds:
    - Claim model
    - XAIOParsed model

    We keep field names aligned to your SCF field names where possible.
    """

    class Claim(BaseModel):
        claim_text: str = Field(..., description="One atomic, checkable statement from the content.")
        claim_type: str = Field(..., description=f"One of: {enums['claim_type']}")


    class XAIOParsed(BaseModel):
        # Core identity / taxonomy
        canonical_url: str
        content_mode: str = Field(..., description=f"Choose from: {enums['content_mode']}")
        language: str = Field(..., description=f"Choose from: {enums['language']}")
        published_at: Optional[str] = Field(None, description="ISO 8601 when available, else null.")

        workflow_status: Optional[str] = Field(None, description=f"Choose from: {enums['workflow_status']} or null.")
        intake_kind: Optional[str] = Field(None, description=f"Choose from: {enums['intake_kind']} or null.")

        # “Paste Bin” / narrative output (your later parsing/fact-check pipeline can use this)
        paste_bin: Optional[str] = Field(
            None,
            description="A clean narrative describing what the text says (not fact-checking).",
        )

        # Keep the full extracted text for WP field (we will also enforce it in code)
        extracted_text_full: str = Field(..., description="Must be identical to input.content.extracted_text_full")

        # Claims list (unverified)
        claims: List[Claim] = Field(default_factory=list)

        # AIO alignment (0–5 or 0–100 per your convention; we’ll use 0–100 to keep simple)
        aio_retrievability: Optional[float] = None
        aio_metadata: Optional[float] = None
        aio_structure: Optional[float] = None
        aio_traceability: Optional[float] = None
        aio_total_score: Optional[float] = None
        aio_tier: Optional[str] = None
        aio_prompt_set_id: Optional[str] = None

        # xAIO modeling scores (0–100). These are about writing quality/traceability, not truth.
        evidence_traceability_score: Optional[float] = None
        evidence_quality: Optional[float] = None
        inference_discipline_score: Optional[float] = None
        claim_explicitness_score: Optional[float] = None
        temporal_precision_score: Optional[float] = None
        information_density_score: Optional[float] = None
        rhetorical_load_score: Optional[float] = None
        machine_readability_score: Optional[float] = None
        internal_consistency_score: Optional[float] = None
        content_integrity_score: Optional[float] = None
        domain: Optional[str] = None
        site_name: Optional[str] = None
        modified_time: Optional[str] = None
        collected_at_utc: Optional[str] = None
        char_count: Optional[int] = None
        word_count: Optional[int] = None

        

    return Claim, XAIOParsed


# ---------------------------
# OpenAI call
# ---------------------------

SYSTEM_PROMPT = """You are xAIO Extractor.

Return ONLY valid JSON matching the provided schema (strict).
Do not include markdown, commentary, or extra keys.

Rules:
- Choose content_mode and language only from the allowed values.
- extracted_text must match the input extracted_text_full VERBATIM.
- Create claims as atomic, checkable statements (one claim per statement).
- Do NOT fact-check. Do NOT output verdict/confidence fields (those happen in a later pipeline).
- Scores are about clarity/traceability/structure (not truth).
"""

@retry(
    reraise=True,
    stop=stop_after_attempt(5),
    wait=wait_exponential(multiplier=1, min=1, max=10),
    retry=retry_if_exception_type((RateLimitError, APITimeoutError, APIConnectionError, APIError)),
    before_sleep=before_sleep_log(logger, logging.WARNING),
)
def call_openai_structured(
    model: str,
    xaio_model: Type[BaseModel],
    ai_input: Dict[str, Any],
    reasoning_effort: Optional[str] = None,
) -> Tuple[Optional[BaseModel], Dict[str, Any]]:
    """
    Returns (parsed_object_or_none, raw_response_dict)
    """
    client = OpenAI()
    return structured_parse(
        client,
        model=model,
        system_prompt=SYSTEM_PROMPT,
        user_content=json.dumps(ai_input, ensure_ascii=False),
        schema=xaio_model,
        reasoning_effort=reasoning_effort,
    )


# ---------------------------
# Post-processing / normalization
# ---------------------------

def postprocess(parsed: BaseModel, ai_input: Dict[str, Any]) -> Dict[str, Any]:
    out = parsed.model_dump()

    full_text = ai_input.get("content", {}).get("extracted_text_full", "")
    out["extracted_text_full"] = full_text


    # Normalize claims
    claims_out = []
    for c in out.get("claims", []) or []:
        ct = normalize_claim_text(c.get("claim_text", ""))
        if not ct:
            continue
        c["claim_text"] = ct
        claims_out.append(c)
    out["claims"] = claims_out

    # Clamp scores
    score_fields = [
        "aio_retrievability","aio_metadata","aio_structure","aio_traceability","aio_total_score",
        "evidence_traceability_score","evidence_quality","inference_discipline_score","claim_explicitness_score",
        "temporal_precision_score","information_density_score","rhetorical_load_score","machine_readability_score",
        "internal_consistency_score","content_integrity_score",
    ]
    for k in score_fields:
        out[k] = clamp_score(out.get(k))

    # If prompt_set_id exists in input, stamp it
    psid = ai_input.get("instructions", {}).get("prompt_set_id")
    if psid:
        out["aio_prompt_set_id"] = psid

    return out


# ---------------------------
# CLI
# ---------------------------

def main() -> int:
    ap = argparse.ArgumentParser(description="Call OpenAI to parse ai_input.json into xaio_parsed.json (Structured Outputs).")
    ap.add_argument("ai_input_json", help="Path to *.ai_input.json produced by reduce4ai.py")
    ap.add_argument("--outdir", default="./out_xaio", help="Where to write *.xaio_parsed.json")
    ap.add_argument("--write-raw", action="store_true", help="Also write a *.xaio_response_raw.json debugging file")
    ap.add_argument("--model", default="gpt-5-nano", help="OpenAI model to use (default: gpt-5-nano)")
    ap.add_argument("--reasoning-effort", default="minimal", help="Reasoning effort for GPT-5 models (none|minimal|low|medium|high|xhigh).")
    ap.add_argument("--scf-export", default="config/scf-export-content.json", help="Path to SCF export JSON (for enum choices).")

    args = ap.parse_args()

    ai_input_path = Path(args.ai_input_json).expanduser().resolve()
    if not ai_input_path.exists():
        raise FileNotFoundError(f"ai_input not found: {ai_input_path}")

    scf_path = Path(args.scf_export).expanduser().resolve()
    if not scf_path.exists():
        raise FileNotFoundError(
            f"SCF export not found: {scf_path}\n"
            f"Tip: copy your SCF export JSON to ~/url-agent/scf-export-2026-01-21.json or pass --scf-export /full/path.json"
        )

    outdir = Path(args.outdir).expanduser().resolve()
    outdir.mkdir(parents=True, exist_ok=True)

    item_id = ai_input_path.stem.replace(".ai_input", "")
    start_time = time.monotonic()
    log_event(logger, stage="parse_start", item_id=item_id, message="parse starting")

    try:
        ai_input = load_json(ai_input_path)
        enums = scf_enums_from_export(scf_path)

        _, XAIOParsed = build_models(enums)

        parsed_obj, raw = call_openai_structured(
            model=args.model,
            xaio_model=XAIOParsed,
            ai_input=ai_input,
            reasoning_effort=args.reasoning_effort if args.reasoning_effort else None,
        )

        # If the model refused, parsed_obj may be None
        if parsed_obj is None:
            raw_path = outdir / (ai_input_path.stem.replace(".ai_input", "") + ".xaio_response_raw.json")
            raw_path.write_text(json.dumps(raw, ensure_ascii=False, indent=2), encoding="utf-8")
            raise RuntimeError("Model refusal or parse failure. Wrote raw response for inspection:\n" + str(raw_path))

        out = postprocess(parsed_obj, ai_input)

        base = ai_input_path.stem.replace(".ai_input", "")
        out_path = outdir / f"{base}.xaio_parsed.json"
        out_path.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding="utf-8")

        if args.write_raw:
            raw_path = outdir / f"{base}.xaio_response_raw.json"
            raw_path.write_text(json.dumps(raw, ensure_ascii=False, indent=2), encoding="utf-8")

        log_event(
            logger,
            stage="parse_done",
            item_id=item_id,
            elapsed_ms_value=elapsed_ms(start_time),
            message=f"wrote={out_path} claims={len(out.get('claims', []))}",
        )
        return 0
    except Exception as exc:
        log_event(
            logger,
            stage="parse_failed",
            item_id=item_id,
            elapsed_ms_value=elapsed_ms(start_time),
            message=f"{type(exc).__name__}: {exc}",
            level=logging.ERROR,
        )
        return 1


if __name__ == "__main__":
    raise SystemExit(main())
